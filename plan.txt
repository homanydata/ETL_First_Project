PROJECT OVERVIEW:
    - postgresql for database, 3 tables, dim_habits, dim_users, fact_records, staging_table
    - telegram bot built using python telebot library that receives new records
    - python script that handles new records and initiates migration
    - flyway migration to create/update warehouse tables (dim tables and agg_tables and views) using psycopg2 library
    - a dashboard in powerbi is connected to views and automatically refreshed

BOT:
- when initiated:
    - a bot is created with token and db_session attributes from Enums in lookups module
    - migration is scheduled on regular basis (time is specified from lookups module)
- commands:
    - start: sends bot description and use
    - record: starts recording process, asks and reads habit, duration, and records user and timestamp, adds that to staging table
    - add category adds a new category
    - week/month/year: sends text message summary of this week/month/year
    - alldata: asks for confirmation and sends excel file of all of this user records

DATABASE HANDLER:
offers basic methods to connect to db:
    - create connection
    - refresh connection (will be used after every migration)
    - close connection
    - execute query a helper method for any query needs to be executed
    - get query result (helper method)
    - get query result as pandas dataframe
    - add new category to dim_category because it is independent from others
    - get distinct categories
    - get distinct habits
    - get habit category (to simplify recordig process with user)

KEYS:
    all sensitive enums will be saved in .env file and accessed using python-dotenv library
    (last step after the project is finished and tested)

LOOKUPS:
    messages, erros outputs, and any needed saved values will be here

MIGRATION:
for flyway migration we will only use the logic, loop over sql_files and run them in order, which are:
    - V0: create schema
    - V1: create staging table
    - V2: insert record into staging table, it contains habit_name, user_id and all its details, duration, timestamp
    - V3: create dim_category, dim_habit & dim_user
    - V4: if new record contains a new habit or new user, add that to dim_habits or dim_users accordingly
    - V5: create fact table
    - V6: add record to fact_records table with habit_id and user_id needed
    - V7: create agg_daily table
    - V8: update agg_daily (or V9 if it is a test)
    - V10: create views
    - V11: truncate staging table

DASHBOARD:
we will create a real-time dashboard that shows:
    - a line graph of records over time
    - line graph of number of active users over time
    - line graph of total duration over time
    - a bar chart of favourite habits
with a slicer by users full names and slicer by habit_names

VIEWS:
what views do we need then?
    - for the 3 line graphs, all get their data from same table, daily_view,
    which includes daily total records and total_duration for each user for each habit
    - for the last bar graph we need a view that shows for each habit total records and total_duration for each user


so in a nutshell: 1 aggregate table is enough, agg_daily
based on this agg tables we will create 2 views: daily_view & habit_summary_view
and based on those views we will create our real-time dashboard

EXCEL HANDLER:
offers helper methods for:
    - creating
    - writing
    - reading
    - deleting excel files

PANDAS HANDLER:
offers basic methods to get specific results for users:
    - return query as excel
    - return query as text
    - get user records
    - get user this day/week/month/year summary
it uses prepared summaries sql commands, and helper methods in excel_handler

SUMMARIES SQL COMMANDS:
list of sql commands used to get a summary for a user

MAIN:
is_test variable is used for making some changes needed for testing, like adding sample data,
changing aggregating query, reducing migration interval...
create a bot instance
an independent thread is created to run any pending job in schedule
run the bot instance