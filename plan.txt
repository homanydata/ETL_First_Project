PROJECT OVERVIEW:
    - postgresql for database, 3 tables, dim_habits, dim_users, fact_records, staging_table
    - telegram bot built using python telebot library that receives new records
    - python script that handles new records and initiates migration
    - flyway migration to create/update warehouse tables (dim tables and agg_tables and views) using psycopg2 library
    - a dashboard in powerbi is connected to views
    - fast brief text summaries or plot images can be provided through telegram bot (pandas, matplotlib & excel in background)

BOT:
- when initiated:
    - a bot is created with token and db_session attributes from Enums in lookups module
    - migration is scheduled on regular basis (time is specified from lookups module)
- commands:
    - start: sends bot description and use
    - record: starts recording process, asks and reads habit, duration, and records user and timestamp, adds that to staging table
    - add category adds a new category
    - day/week/month/year: sends text message summary of this week/month/year
    - alldata: asks for confirmation and sends excel file of all of this user records
    - summarize: chooses a habit and gets a plot image summary for it

DATABASE HANDLER:
offers basic methods to connect to db:
    - create connection
    - refresh connection (will be used after every migration)
    - close connection
    - execute query a helper method for any query needs to be executed
    - get query result (helper method)
    - get query result as pandas dataframe
    - add new category to dim_category because it is independent from others
    - get distinct categories
    - get distinct habits
    - get habit category (to simplify recordig process with user)

KEYS:
    all sensitive enums will be saved in .env file and accessed using python-dotenv library
    (last step after the project is finished and tested)

LOOKUPS:
    messages, markups, enums, erros outputs, directories... and any needed saved values will be here

MIGRATION:
for flyway migration we will only use the logic, loop over sql_files and run them in order, which are:
    - V0: create schema
    - V1: create staging table
    - V2: insert record into staging table, it contains habit_name, user_id and all its details, duration, timestamp
    - V3: create dim_category, dim_habit & dim_user
    - V4: if new record contains a new habit or new user, add that to dim_habits or dim_users accordingly
    - V5: create fact table
    - V6: add record to fact_records table with habit_id and user_id needed
    - V7: create agg_daily table
    - V8: update agg_daily (or V9 if it is a test)
    - V10: create views
    - V11: truncate staging table

DASHBOARD:
we will create a real-time dashboard that shows:
    - line graph of number of active users over time
    - line graph of total duration over time
    - a bar chart of favourite habits
    - a pie chart of categories
with a slicer by users full names

VIEWS:
what views do we need then?
    - for the 3 line graphs, all get their data from same table, daily_view,
    which includes daily total records and total_duration for each user for each habit
    - for the last bar graph we need a view that shows for each habit total records and total_duration for each user


so in a nutshell: 1 aggregate table is enough, agg_daily
based on this agg tables we will create 2 views: daily_view & habit_summary_view
and based on those views we will create our real-time dashboard

PANDAS HANDLER:
offers basic methods to get specific results for users:
    - return query as text
    - get user records
    - get user this day/week/month/year summary
    - get plot image (runs query to get needed df, then, uses create_plot_image from plot_creator module to return plot image)
it uses prepared summaries sql commands

Plot Creator:
it is goal is to use matplotlib to create plots from 2 columns dataframes and return it as image
    - has general create_plot_image method that uses either create_bar_chart or create_line_graph method
    - then return_plot_as_image method to change a dataframe to image of a plot

Excel Handler:
    - df to excel method
    - delete excel file method

SUMMARIES SQL COMMANDS:
needed sql queries to get data for any type of summary for user
    - all user records
    - interval summary (this day/week/month/year): results grouped by habits over a specific interval
    - variable summary
    - variable type parts distribution (totals for each habit/category)

MAIN:
is_test variable is used for making some changes needed for testing, like adding sample data,
changing aggregating query, reducing migration interval...
create a bot instance
an independent thread is created to run any pending job in schedule
run the bot instance